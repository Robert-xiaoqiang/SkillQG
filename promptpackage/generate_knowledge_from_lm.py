from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM

class CausalGenerator:
    def __init__(self, pretrained_model_name_or_path = 'gpt2', device = None):
        if device is None:
            self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        model_config = AutoConfig.from_pretrained(pretrained_model_name_or_path)
        self.tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path,
            do_lower_case = True
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path,
            config = model_config
        )

        self.CONFIG_FILLING_QUESTION_PREFIX = {
            'NUM_ADDITIONAL_TOKENS': 6,
            'TOP_P': 0,
            'NUM_BEAMS': 0,
            'NUM_RETURN_SEQUENCES': 0,
        }

        self.CONFIG_FILLING_ANSWER_PREFIX = {
            'NUM_ADDITIONAL_TOKENS': 10,
            'TOP_P': 0,
            'NUM_BEAMS': 0,
            'NUM_RETURN_SEQUENCES': 0,
        }

        self.model.eval()
        self.model.to(self.device)

    def build_input(self, context, others):
        blank_or_space_token = ' '
        test_context = tokenizer.bos_token +
                       blank_or_space_token + context +
                       blank_or_space_token + others
        
        # dict of a single sample
        test_input = tokenizer(test_context, padding = False, truncation = False, return_attention_mask = True, return_token_type_ids = True, return_tensors = 'pt')

        # wrap the single sample with a batch dimension and move it to the same device as the model parameters
        for key, value in test_input.items():
            test_input[key] = value.unsqueeze(dim = 0).to(self.device)
        
        return test_input

    def generate_additional_tokens(self, test_input, num_additional_tokens, top_p, num_beams):
        input_ids = test_input['input_ids']
        raw_length = input_ids.size(1)
        max_length = raw_length + num_additional_tokens

        sample_outputs = self.model.generate(
            input_ids = input_ids,
            max_length = max_length,
            early_stopping = True,
            temperature = 0.85,
            do_sample = True,
            top_p = top_p,
            num_beams = num_beams,
            num_return_sequences = 1,

            # output_scores = output_scores,
            # output_hidden_states = output_hidden_states,
            # output_attentions = output_attentions,

            bos_token_id = self.tokenizer.bos_token_id,
            pad_token_id = self.tokenizer.pad_token_id,
            eos_token_id = self.tokenizer.eos_token_id,

            return_dict_in_generate = True
        )
        # BeamSampleDecoderOnlyOutput (sequences, sequences_scores, scores, beam_indices, attentions, hidden_states)
        # BeamSampleEncoderDecoderOutput 6 + 3 = 9
        # batch_size*num_return_sequences x max_length
        sample_ids = sample_outputs['sequences']
        
        sample_sequence = self.tokenizer.batch_decode(sample_ids[:, raw_length:], skip_special_tokens = True)

        additional_tokens = sample_sequence.strip()
        
        return additional_tokens

    '''
        Generate additional tokens for the quetion prefix.

        Args:
            - `context` (str) : input context of the question generation task, usually including the paragraph, answer, prerequisite skill, and the filling results of previous steps
            - `question_prefix` (str) : skill- or level-specific prompt

        Return:
            - `full_question` (str) : a full question starting with the given prefix and ending with a question mark
            - `additional_tokens_filling_question_prefix` : additional tokens generated by the language model
    '''
    def fill_question_prefix_individual(self, context, question_prefix):
        test_input = self.build_input(context, question_prefix)
        addition_tokens = self.generate_additional_tokens(test_input)

    def fill_answer_prefix_individual(self, context, answer_prefix):
        pass

    def fill_question_prefix_batch(self, context, question_prefix):
        pass

    def fill_answer_prefix_batch(self, context, answer_prefix):
        pass