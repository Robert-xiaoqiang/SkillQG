import os
import json
import torch
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
import spacy


class CausalGenerator:
    def __init__(self, pretrained_model_name_or_path = 'gpt2', device = None):
        if device is None:
            self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        model_config = AutoConfig.from_pretrained(pretrained_model_name_or_path)
        self.tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path,
            do_lower_case = True
        )
        self.tokenizer.add_special_tokens({
            'pad_token': '<pad>'
        })
        self.model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path,
            config = model_config
        )
        
        self.CONFIG_FILLING_QUESTION_PREFIX = {
            'NUM_ADDITIONAL_TOKENS': 6,
            'TOP_P': 0.2, # sampling from the top 20% tokens
            'NUM_BEAMS': 8,
            'NUM_RETURN_SEQUENCES': 1
        }

        self.CONFIG_FILLING_ANSWER_PREFIX = {
            'NUM_ADDITIONAL_TOKENS': 10,
            'TOP_P': 0.5,
            'NUM_BEAMS': 8,
            'NUM_RETURN_SEQUENCES': 1
        }

        self.model.eval()
        self.model.to(self.device)

    def build_input(self, context, others):
        blank_or_space_token = ' '
        test_context = self.tokenizer.bos_token +\
                       blank_or_space_token + context +\
                       blank_or_space_token + others
        
        # The dict of a single sample will also be wrapped with a batch dimension automatically by the tokenizer.__call__(...)
        test_input = self.tokenizer(test_context, padding = False, truncation = False, return_attention_mask = True, return_token_type_ids = True, return_tensors = 'pt')
        
        # move it to the same device as the model parameters
        for key, value in test_input.items():
            test_input[key] = value.to(self.device)
        
        return test_input

    def generate_additional_tokens(self, test_input, num_additional_tokens, top_p, num_beams, num_return_sequences):
        input_ids = test_input['input_ids']
        raw_length = input_ids.size(1) # B x max_seq_length (a.k.a. actual length of this single sample in this batch) x (optional embdim)
        max_length = raw_length + num_additional_tokens

        sample_outputs = self.model.generate(
            input_ids = input_ids,
            max_length = max_length,
            early_stopping = True,
            temperature = 0.85,
            do_sample = True,
            top_p = top_p,
            num_beams = num_beams,
            num_return_sequences = num_return_sequences,

            # output_scores = output_scores,
            # output_hidden_states = output_hidden_states,
            # output_attentions = output_attentions,

            bos_token_id = self.tokenizer.bos_token_id,
            pad_token_id = self.tokenizer.pad_token_id,
            eos_token_id = self.tokenizer.eos_token_id,

            return_dict_in_generate = True
        )
        # BeamSampleDecoderOnlyOutput (sequences, sequences_scores, scores, beam_indices, attentions, hidden_states)
        # BeamSampleEncoderDecoderOutput 6 + 3 = 9
        # batch_size*num_return_sequences x max_length
        sample_ids = sample_outputs['sequences']
        
        sample_sequences = self.tokenizer.batch_decode(sample_ids[:, raw_length:], skip_special_tokens = True)

        # left and right strip
        additional_tokens_list = [ s.strip() for s in sample_sequences ]
        
        return additional_tokens_list

    '''
        Generate additional tokens for the quetion prefix.

        Args:
            - `context` (str) : input context of the question generation task, usually including the paragraph, answer, prerequisite skill, and the filling results of previous steps
            - `question_prefix` (str) : skill- or level-specific prompt

        Return:
            - `full_question` (str) : a full question starting with the given prefix and ending with a question mark
            - `additional_tokens_filling_question_prefix` : additional tokens generated by the language model and stripped the annoying punctuations
    '''
    def fill_question_prefix_individual(self, context, question_prefix):
        test_input = self.build_input(context, question_prefix)
        additional_tokens_list = self.generate_additional_tokens(test_input,
                                                            num_additional_tokens = self.CONFIG_FILLING_QUESTION_PREFIX['NUM_ADDITIONAL_TOKENS'],
                                                            top_p = self.CONFIG_FILLING_QUESTION_PREFIX['TOP_P'],
                                                            num_beams = self.CONFIG_FILLING_QUESTION_PREFIX['NUM_BEAMS'],
                                                            num_return_sequences = self.CONFIG_FILLING_QUESTION_PREFIX['NUM_RETURN_SEQUENCES'])

        def parse_single_question_candidate(additional_tokens):
            stop_index_1, stop_index_2 = additional_tokens.find('?'), additional_tokens.find('\n')
            if stop_index_1 != -1:
                additional_tokens = additional_tokens[:stop_index_1+1] # stop_index_1 + 1 > len(str) is also OK
                additional_tokens_without_punctuations = additional_tokens[:-1]
            elif stop_index_2 != -1:
                additional_tokens = additional_tokens[:stop_index_1]
                additional_tokens_without_punctuations = additional_tokens
            else:
                # additional_tokens is identical to the return str of LM (a.k.a. additional_tokens_filling_question_prefix)
                additional_tokens_without_punctuations = additional_tokens
            
            full_question = question_prefix + ' ' + additional_tokens
            full_question = full_question + '?' if not full_question.endswith('?') else full_question

            return full_question, additional_tokens_without_punctuations

        ret = [ parse_single_question_candidate(ad) for ad in additional_tokens_list ]
        # list of tuples of full_question and additional_tokens_without_punctuations
        return ret

    def fill_answer_prefix_individual(self, context, answer_prefix):
        test_input = self.build_input(context, answer_prefix)
        additional_tokens_list = self.generate_additional_tokens(test_input,
                                                            num_additional_tokens = self.CONFIG_FILLING_ANSWER_PREFIX['NUM_ADDITIONAL_TOKENS'],
                                                            top_p = self.CONFIG_FILLING_ANSWER_PREFIX['TOP_P'],
                                                            num_beams = self.CONFIG_FILLING_ANSWER_PREFIX['NUM_BEAMS'],
                                                            num_return_sequences = self.CONFIG_FILLING_ANSWER_PREFIX['NUM_RETURN_SEQUENCES'])

        def parse_single_answer_candidate(additional_tokens):
            stop_index_1, stop_index_2 = additional_tokens.find('.'), additional_tokens.find('\n')
            if stop_index_1 != -1:
                additional_tokens = additional_tokens[:stop_index_1+1] # stop_index_1 + 1 > len(str) is also OK
                additional_tokens_without_punctuations = additional_tokens[:-1]
            elif stop_index_2 != -1:
                additional_tokens = additional_tokens[:stop_index_1]
                additional_tokens_without_punctuations = additional_tokens
            else:
                # additional_tokens is identical to the return str of LM (a.k.a. additional_tokens_filling_answer_prefix)
                additional_tokens_without_punctuations = additional_tokens            

            full_answer = answer_prefix + ' ' + additional_tokens
            full_answer = full_answer + '.' if not full_answer.endswith('.') else full_answer

            return full_answer, additional_tokens_without_punctuations

        ret = [ parse_single_answer_candidate(ad) for ad in additional_tokens_list ]
        # list of tuples of full_answer and additional_tokens_without_punctuations
        return ret

    def fill_question_prefix_batch(self, context, question_prefix):
        pass

    def fill_answer_prefix_batch(self, context, answer_prefix):
        pass


class KnowledgePrompter:
    def __init__(self, prompt_filename):
        # json.load() will read all the content of the file into memeory and we can discard the corresponding file handler
        with open(prompt_filename) as f:
            self.prompt_dict = json.load(f)
        
        self.prompt_mapping = {
            'character': 'remember',
            'setting': 'remember',
            'feeling': 'understand',
            'action': 'apply',
            'causal relationship': 'analyze',
            'outcome resolution': 'analyze',
            'prediction': 'create'
        }

        self.generator = CausalGenerator()

        self.nlp = spacy.load('en_core_web_sm')

    def build_sample(self, sample_entry):
        c, a, r = sample_entry['context'], sample_entry['answer'], sample_entry['reasoning_skill']
        bloom_key = self.prompt_mapping[r]
        prompt_list = self.prompt_dict[bloom_key]

        ret_dict = dict(sample_entry)

        for prompt_index, prompt_entry in enumerate(prompt_list):
            prompt_key = 'prompt-#{}'.format(prompt_index)
            ret_dict[prompt_key] = { }

            if prompt_entry['question-prefix-input'] is None:
                assert prompt_entry['question-prefix-filling'] == 'generation' and prompt_entry['answer-prefix-input'] == 'additional-tokens-filling-question-prefix' and prompt_entry['answer-prefix-filling'] == 'generation', 'Error in the schema of bloom prompt'

                sample_extra_dict = self.build_sample_generation(sample_entry, prompt_entry)
            elif prompt_entry['question-prefix-input'] == 'entity-tagging':
                assert prompt_entry['question-prefix-filling'] is None and prompt_entry['answer-prefix-input'] == 'entity-tagging' and prompt_entry['answer-prefix-filling'] == 'generation', 'Error in the schema of bloom prompt'

                sample_extra_dict = self.build_sample_entity_tagging(sample_entry, prompt_entry)
            else:
                # raise NotImplementedError('Error in the prompt entry of prompt file')
                sample_extra_dict = { }
            
            # map from `prompt_key` (`prompt_id`) to prompted things of current sample (`extra_knowledge_specific_to_this_sample`)
            ret_dict[prompt_key] = sample_extra_dict

        return ret_dict

    def build_sample_generation(self, sample_entry, prompt_entry):
        '''
            first build the following dict
            {
                talking-#n: {
                    question: xx,
                    answers: [

                    ]
                }
                ...
                ...
            }
            and then merge the return value (typed dict) with the `ret_dict[prompt_key]`.
        '''
        ret = { }
        c, a, r = sample_entry['context'], sample_entry['answer'], sample_entry['reasoning_skill']
        context = c + ' ' + a

        question_prefix = prompt_entry['question-prefix']
        incomplete_answer_prefix = prompt_entry['answer-prefix']

        questions_list = self.generator.fill_question_prefix_individual(context, question_prefix)

        for question_id, (full_question, additional_tokens) in enumerate(questions_list):
            question_key = 'talking-#{}'.format(question_id)
            ret[question_key] = {
                'question': full_question,
                'answers': [ ]
            }

            new_context = context + ' ' + full_question
            answer_prefix = incomplete_answer_prefix.replace('<INPUT>', additional_tokens)
            answers_list = self.generator.fill_answer_prefix_individual(new_context, answer_prefix)

            for full_answer, _ in answers_list:
                ret[question_key]['answers'].append(full_answer)

        return ret 

    def build_sample_entity_tagging(self, sample_entry, prompt_entry):
        '''
            first build the following dict
            {
                talking-#n: {
                    question: xx,
                    answers: [

                    ]
                }
                ...
                ...
            }
            and then merge the return value (typed dict) with the `ret_dict[prompt_key]`.
        '''
        ret = { }
        c, a, r = sample_entry['context'], sample_entry['answer'], sample_entry['reasoning_skill']
        context = c + ' ' + a

        question_prefix = prompt_entry['question-prefix']
        incomplete_answer_prefix = prompt_entry['answer-prefix']

        # substitute the <INPUT> token of the question prefix with the entity tagging (SUBJECT only)
        substitution_candidates = list(
            map(lambda t: t.text,
                filter(lambda t: 'subj' in t.dep_,
                    self.nlp(a)
                )
            )
        )

        for question_id, substitution_candidate in enumerate(substitution_candidates):
            question_key = 'talking-#{}'.format(question_id)
            full_question = question_prefix.replace('<INPUT>', substitution_candidate)
            ret[question_key] = {
                'question': full_question,
                'answers': [ ]
            }

            new_context = context + ' ' + full_question
            answer_prefix = incomplete_answer_prefix.replace('<INPUT>', substitution_candidate)
            answers_list = self.generator.fill_answer_prefix_individual(new_context, answer_prefix)

            for full_answer, _ in answers_list:
                ret[question_key]['answers'].append(full_answer)

        return ret

    def build_dataset(self, source_dirname, splits, target_dirname):
        for split in splits:
            source_filename = os.path.join(source_dirname, '{}.json'.format(split))
            os.makedirs(target_dirname, exist_ok = True) # -p
            with open(source_filename) as f:
                source_json = json.load(f)
            
            target_dict = { } 
            # count = 0
            for qid, sample_entry in source_json.items():
                # count += 1
                # if count == 16:
                #     break
                target_dict[qid] = self.build_sample(sample_entry)

            target_filename = os.path.join(target_dirname, '{}.json'.format(split))
            with open(target_filename, 'w') as f:
                json.dump(target_dict, f, indent = 4)

# if __name__ == '__main__':
#     import sys
#     import argparse
#     sys.path.insert(1, '/home/xqwang/projects/qgqa/skill-qg/')
#     from skillqgpackage.data.FairytaleQADataset import FairytaleQASeq2SeqLMDataset
#     from configure.default import config, update_config
#     parser = argparse.ArgumentParser(description='Train segmentation network')
    
#     parser.add_argument('--cfg',
#                         help='experiment configure file name',
#                         type=str)
#     parser.add_argument('opts',
#                         help="Modify config options using the command-line interface",
#                         default=None,
#                         nargs=argparse.REMAINDER)

#     args = parser.parse_args()
#     update_config(config, args)
#     # d = FairytaleQASeq2SeqLMDataset(config, 'train', 'dummy')
#     # d.parse_and_build()
#     d = FairytaleQASeq2SeqLMDataset(config, 'val', 'dummy')
#     d.parse_and_build()
#     # d = FairytaleQASeq2SeqLMDataset(config, 'test', 'dummy')
#     # d.parse_and_build()


if __name__ == '__main__':
    source_dirname = '/home/xqwang/projects/qgqa/FairytaleQAData/json'
    target_dirname = '/home/xqwang/projects/qgqa/FairytaleQAData/lm-augmented-json'
    splits = [ 'train', 'val', 'test' ]

    prompt_filename = '/home/xqwang/projects/qgqa/skill-qg/promptpackage/fairytaleqa_bloom_prompt.json'

    prompter = KnowledgePrompter(prompt_filename)
    prompter.build_dataset(source_dirname, splits, target_dirname)
