import os
import json
import torch
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM

class CausalGenerator:
    def __init__(self, pretrained_model_name_or_path = 'gpt2', device = None):
        if device is None:
            self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        model_config = AutoConfig.from_pretrained(pretrained_model_name_or_path)
        self.tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path,
            do_lower_case = True
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path,
            config = model_config
        )

        self.CONFIG_FILLING_QUESTION_PREFIX = {
            'NUM_ADDITIONAL_TOKENS': 6,
            'TOP_P': 0.2, # sampling from the top 20% tokens
            'NUM_BEAMS': 8,
            'NUM_RETURN_SEQUENCES': 2
        }

        self.CONFIG_FILLING_ANSWER_PREFIX = {
            'NUM_ADDITIONAL_TOKENS': 10,
            'TOP_P': 0.5,
            'NUM_BEAMS': 8,
            'NUM_RETURN_SEQUENCES': 2
        }

        self.model.eval()
        self.model.to(self.device)

    def build_input(self, context, others):
        blank_or_space_token = ' '
        test_context = self.tokenizer.bos_token +\
                       blank_or_space_token + context +\
                       blank_or_space_token + others
        
        # The dict of a single sample will also be wrapped with a batch dimension automatically by the tokenizer.__call__(...)
        test_input = self.tokenizer(test_context, padding = False, truncation = False, return_attention_mask = True, return_token_type_ids = True, return_tensors = 'pt')
        
        # move it to the same device as the model parameters
        for key, value in test_input.items():
            test_input[key] = value.to(self.device)
        
        return test_input

    def generate_additional_tokens(self, test_input, num_additional_tokens, top_p, num_beams, num_return_sequences):
        input_ids = test_input['input_ids']
        raw_length = input_ids.size(1) # B x max_seq_length (a.k.a. actual length of this single sample in this batch) x (optional embdim)
        max_length = raw_length + num_additional_tokens

        sample_outputs = self.model.generate(
            input_ids = input_ids,
            max_length = max_length,
            early_stopping = True,
            temperature = 0.85,
            do_sample = True,
            top_p = top_p,
            num_beams = num_beams,
            num_return_sequences = num_return_sequences,

            # output_scores = output_scores,
            # output_hidden_states = output_hidden_states,
            # output_attentions = output_attentions,

            bos_token_id = self.tokenizer.bos_token_id,
            pad_token_id = self.tokenizer.pad_token_id,
            eos_token_id = self.tokenizer.eos_token_id,

            return_dict_in_generate = True
        )
        # BeamSampleDecoderOnlyOutput (sequences, sequences_scores, scores, beam_indices, attentions, hidden_states)
        # BeamSampleEncoderDecoderOutput 6 + 3 = 9
        # batch_size*num_return_sequences x max_length
        sample_ids = sample_outputs['sequences']
        
        sample_sequences = self.tokenizer.batch_decode(sample_ids[:, raw_length:], skip_special_tokens = True)

        # left and right strip
        additional_tokens_list = [ s.strip() for s in sample_sequences ]
        
        return additional_tokens_list

    '''
        Generate additional tokens for the quetion prefix.

        Args:
            - `context` (str) : input context of the question generation task, usually including the paragraph, answer, prerequisite skill, and the filling results of previous steps
            - `question_prefix` (str) : skill- or level-specific prompt

        Return:
            - `full_question` (str) : a full question starting with the given prefix and ending with a question mark
            - `additional_tokens_filling_question_prefix` : additional tokens generated by the language model and stripped the annoying punctuations
    '''
    def fill_question_prefix_individual(self, context, question_prefix):
        test_input = self.build_input(context, question_prefix)
        additional_tokens_list = self.generate_additional_tokens(test_input,
                                                            num_additional_tokens = self.CONFIG_FILLING_QUESTION_PREFIX['NUM_ADDITIONAL_TOKENS'],
                                                            top_p = self.CONFIG_FILLING_QUESTION_PREFIX['TOP_P'],
                                                            num_beams = self.CONFIG_FILLING_QUESTION_PREFIX['NUM_BEAMS'],
                                                            num_return_sequences = self.CONFIG_FILLING_QUESTION_PREFIX['NUM_RETURN_SEQUENCES'])

        def parse_single_question_candidate(additional_tokens):
            stop_index_1, stop_index_2 = additional_tokens.find('?'), additional_tokens.find('\n')
            if stop_index_1 != -1:
                additional_tokens = additional_tokens[:stop_index_1+1] # stop_index_1 + 1 > len(str) is also OK
                additional_tokens_without_punctuations = additional_tokens[:-1]
            elif stop_index_2 != -1:
                additional_tokens = additional_tokens[:stop_index_1]
                additional_tokens_without_punctuations = additional_tokens
            else:
                # additional_tokens is identical to the return str of LM (a.k.a. additional_tokens_filling_question_prefix)
                additional_tokens_without_punctuations = additional_tokens
            
            full_question = question_prefix + ' ' + additional_tokens
            full_question = full_question + '?' if not full_question.endswith('?') else full_question

            return full_question, additional_tokens_without_punctuations

        ret = [ parse_single_question_candidate(ad) for ad in additional_tokens_list ]
        # list of tuples of full_question and additional_tokens_without_punctuations
        return ret

    def fill_answer_prefix_individual(self, context, answer_prefix):
        test_input = self.build_input(context, answer_prefix)
        additional_tokens_list = self.generate_additional_tokens(test_input,
                                                            num_additional_tokens = self.CONFIG_FILLING_ANSWER_PREFIX['NUM_ADDITIONAL_TOKENS'],
                                                            top_p = self.CONFIG_FILLING_ANSWER_PREFIX['TOP_P'],
                                                            num_beams = self.CONFIG_FILLING_ANSWER_PREFIX['NUM_BEAMS'],
                                                            num_return_sequences = self.CONFIG_FILLING_ANSWER_PREFIX['NUM_RETURN_SEQUENCES'])

        def parse_single_answer_candidate(additional_tokens):
            stop_index_1, stop_index_2 = additional_tokens.find('.'), additional_tokens.find('\n')
            if stop_index_1 != -1:
                additional_tokens = additional_tokens[:stop_index_1+1] # stop_index_1 + 1 > len(str) is also OK
                additional_tokens_without_punctuations = additional_tokens[:-1]
            elif stop_index_2 != -1:
                additional_tokens = additional_tokens[:stop_index_1]
                additional_tokens_without_punctuations = additional_tokens
            else:
                # additional_tokens is identical to the return str of LM (a.k.a. additional_tokens_filling_answer_prefix)
                additional_tokens_without_punctuations = additional_tokens            

            full_answer = answer_prefix + ' ' + additional_tokens
            full_answer = full_answer + '.' if not full_answer.endswith('.') else full_answer

            return full_answer, additional_tokens_without_punctuations

        ret = [ parse_single_answer_candidate(ad) for ad in additional_tokens_list ]
        # list of tuples of full_answer and additional_tokens_without_punctuations
        return ret

    def fill_question_prefix_batch(self, context, question_prefix):
        pass

    def fill_answer_prefix_batch(self, context, answer_prefix):
        pass


def parse_and_build(source_dirname, splits, target_dirname):
    with open('./fairytaleqa_bloom_prompt.json') as f:
        prompt_dict = json.load(f)

    for split in splits:
        source_filename = os.path.join(source_dirname, '{}.json'.format(split))


if __name__ == '__main__':
    # import sys
    # import argparse
    # sys.path.insert(1, '/home/xqwang/projects/qgqa/skill-qg/')
    # from skillqgpackage.data.FairytaleQADataset import FairytaleQASeq2SeqLMDataset
    # from configure.default import config, update_config
    # parser = argparse.ArgumentParser(description='Train segmentation network')
    
    # parser.add_argument('--cfg',
    #                     help='experiment configure file name',
    #                     type=str)
    # parser.add_argument('opts',
    #                     help="Modify config options using the command-line interface",
    #                     default=None,
    #                     nargs=argparse.REMAINDER)

    # args = parser.parse_args()
    # update_config(config, args)

    # d = FairytaleQASeq2SeqLMDataset(config, 'test', 'dummy')
    # d.parse_and_build()

if __name__ == '__main__':
    source_filename = ''
    target_filename = ''

    ret = parse_and_build(source_filename)

    with open(target_filename, 'w') as f:
        json.dump(ret, f, indent = 4)
